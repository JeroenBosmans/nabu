[trainer]
#name of the trainer that should be used
trainer = cross_entropy_text
#if set to True training will resume from latest checkpoint
resume_training = False
#number of passes over the entire database
num_epochs = 50
#initial learning rate of the neural net
initial_learning_rate = 1e-3
#exponential weight decay parameter
learning_rate_decay = 1
#size of the minibatch (#utterances)
batch_size = 8
#number of minibatches to aggregate before updating the parameters if 0
#asstnchronous training will be done
numbatches_to_aggregate = 0
#if there is no dev set a dev set will be created from the training set, this
#sets the number of training utterances that will be used for validation
valid_utt = 16
#The validation mode, options are decode and loss. Decode will decode the
#validation set and compare with the validation targets. Loss will compute the
#loss on the validation set
validation_mode = loss
#frequency of evaluating the validation set. CATION: If the valid_frequency is
#more than 5 times larger than the checkpoint frequency the validated model
#will be deleted by the saver
valid_frequency = 500
#if you want to adapt the learning rate based on the validation set, set to True
valid_adapt = True
# optimizer that is used: 'gradient_descent' or 'adam'
optimizer = adam
# if adam is specified, we can also specify to adapt beta1 and beta2
#beta1 = 0.9
#beta2 = 0.999
# LAS_ignoring_mode should be true or false and holds wether we are ignoring
# examples without text targets or are just making sure their cost is zero
las_ignoring_mode = False
# do we want the batches to be of fixed ratio between labeled and unlabeled
fixed_ratio = True
# learning rate adaptation boolean
learning_rate_adaptation = False
# set this to true to repeat the old method of using unlabeled data in cost function
# old_method_unlabeled = True
